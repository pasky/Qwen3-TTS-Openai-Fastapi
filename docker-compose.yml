version: '3.8'

services:
  # GPU-enabled service with official backend (default)
  qwen3-tts-gpu:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    container_name: qwen3-tts-api
    ports:
      - "8880:8880"
    environment:
      - HOST=0.0.0.0
      - PORT=8880
      - WORKERS=1
      - CORS_ORIGINS=*
      - TTS_BACKEND=official
      - TTS_WARMUP_ON_START=false
    volumes:
      # Mount model cache for persistence
      - ~/.cache/huggingface:/home/appuser/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8880/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # GPU-enabled service with vLLM-Omni backend
  qwen3-tts-vllm:
    build:
      context: .
      dockerfile: Dockerfile
      target: vllm-production
    container_name: qwen3-tts-api-vllm
    ports:
      - "8880:8880"
    environment:
      - HOST=0.0.0.0
      - PORT=8880
      - WORKERS=1
      - CORS_ORIGINS=*
      - TTS_BACKEND=vllm_omni
      - TTS_WARMUP_ON_START=true
      # Use smaller 0.6B model for faster inference
      - TTS_MODEL_NAME=Qwen/Qwen3-TTS-12Hz-0.6B-CustomVoice
    volumes:
      # Mount model cache for persistence
      - ~/.cache/huggingface:/home/appuser/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8880/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    profiles:
      - vllm

  # CPU-only service
  qwen3-tts-cpu:
    build:
      context: .
      dockerfile: Dockerfile
      target: cpu-base
    container_name: qwen3-tts-api-cpu
    ports:
      - "8880:8880"
    environment:
      - HOST=0.0.0.0
      - PORT=8880
      - WORKERS=1
      - CORS_ORIGINS=*
      - TTS_BACKEND=official
    volumes:
      - ~/.cache/huggingface:/home/appuser/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8880/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    profiles:
      - cpu

# To run GPU version with official backend: docker-compose up qwen3-tts-gpu
# To run GPU version with vLLM backend: docker-compose --profile vllm up qwen3-tts-vllm
# To run CPU version: docker-compose --profile cpu up qwen3-tts-cpu
